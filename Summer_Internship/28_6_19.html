<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>28_6_19</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
</head>
<body>
<h1 id="day-21-of-summer-internship">Day 21 of Summer Internship</h1>
<p><em>Date: June 28st, 2019</em></p>
<p>Yesterday we had left our process running on two systems, one was my own and the other was the server. I had to eventually stop the process on my system because it was taking up too much memory. But the first thing I did after entering the office today was to check the result of the process, and here it is:</p>
<pre><code>Total mails: 2853
Classified as spam: 2849
Classified as ham: 4
Accuracy: 99.85%
Time for the process to complete: 16hr 31min</code></pre>
<p>Now looking at the results, I was happy but a bit skeptical. 99.85% is way too much for me. I am a BTech student, we are basically like this:</p>
<p><img src="https://i.imgflip.com/34kfae.jpg" /></p>
<p>Logically, there were so many things that could have given this “wrong” result. But for my head, what was more concerning was the time consumed. According to him, 16 and a half hours is far too much time to process just a small of number of emails. My hypothesis for this excessive amount of time was that most of the time is being spend on loading the required modules for each mail. Yeah, each mail was being processed separately, independent of each other. I had basically just called the single mail processor in loop. So for each mail, the ML model, dictionary and spacy modules were being loaded and then unloaded. To verify this hypothesis, I combined all the code in a single file so that the modules will be loaded only once and all the mails could be processed using them. And here is the result:</p>
<pre><code>No. of mails processed: 2853
No. of mails classified as spams: 1738
No. of mails classified as ham: 1115
Accuracy: 60.91%
Time consumed: 3min</code></pre>
<p>And I was right. It was all just loading and unloading work that was slowing us down. It’s really terrifying to realise just how much time could be wasted on file reading operations. And these are truly the things that a computer scientist should worry about. But another thing that’s fallen is the accuracy. And honestly, I am not much surprised. This is how it should be. This is just a more realistic value. But I couldn’t think of a single reason for this change in accuracy. There are far too many factors to consider. For example, if there are no plain or html text’s in a email, our program would classify this as a ham mail. Which is correct I guess. You won’t ignore someone not saying anything. Actually, you can’t because he is the one ignoring you.</p>
<p>Oh by the way, I noticed that many mails have text content but they formatted in html. So I used <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">Beautiful Soup 4</a> to remove the html tags from the text and get the base content. Though beside the <code>get_text()</code> method of bs4, I also had to get the help of this <a href="https://stackoverflow.com/questions/328356/extracting-text-from-html-file-using-python">StackOverflow answer</a> to get the text part properly. I did all this before the above results were acquired, so this may have been a factor too.</p>
<p>So now I think there isn’t much left to do. After I test this model against ham mails and get a satisfactory result, I am planning to integrate this model in the mail server and focus on the continuous improvement part. After we are done with that, just the documentation will be left and we will finally finish this project. But something that will never finish are these recursion jokes:</p>
<p><img src="https://cdn-images-1.medium.com/fit/t/1600/480/1*yNNmaPaMjbto_oSlcO7hvQ.png" /></p>
</body>
</html>
