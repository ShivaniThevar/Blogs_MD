# Day 15 of Summer Internship

*Date: June 21st, 2019*

Yesterday, we had lest the work in search of a better Natural Language Processing library than NLTK. I asked for this in the [GD](http://greatdevelopers.github.io/) and got various links. From one of those links, I found out about the [spaCy library](https://spacy.io/). spaCy is a library for advanced Natural Language Processing in Python and Cython. Many praise it for its industry-ready features. So after doing some research on it, I decided to use it in my preprocessor. 

But deploying spaCy was not as easy as I thought it would be. This was largely due to [tokenization](https://spacy.io/api/tokenizer). Tokenization is essentially breaking the given text into segments which makes further processing simpler. So it was essential, but it forced to largely change the way I was processing and storing the words. But it was all for the betterment of the code so no complaints there. After doing all that, I was getting proper words in the processed text. 

Now it was time for lemmatization. SpaCy has a very convenient function that would give you the lemma (the base form) of the word. But lemmatization, though easy didn't happened exactly as I had thought in the following ways:

1. I was sure that the token.lemma\_ function would return another token, but to my surprise, it returns a string object.

2. Some words I thought should have been turned into their lemma were shown in their original form. For example:

	sudhanshu@dubey:/media/sudhanshu/Thunder/Documents/Bolgs_MD/Summer_Internship$ python3
	Python 3.6.8 (default, Apr  9 2019, 04:59:38) 
	[GCC 8.3.0] on linux
	Type "help", "copyright", "credits" or "license" for more information.
	>>> import spacy
	>>> nlp = spacy.load("en_core_web_sm")
	>>> doc = nlp(u'surprise surprising surprisingly')
	>>> for token in doc:
	...     print(token.lemma_)
	... 
	surprise
	surprising
	surprisingly

I had honestly thought that all the words would be turned to surprise. Was the lemmatizer not working properly? Python said no to this:

	>>> doc1 = nlp(u'write writing written wrote')
	>>> for token in doc1:
	...     print(token.lemma_)
	... 
	write
	write
	write
	write

So I guess I need to improve my concepts of NLP processes. Anyway, now I had my preprocessor ready. I may add other features for better text processing, but that's for later. Currently, there was one thing pestering me. The model building code also used a native for of this preprocessor and so I had to update that too because the data should be processed in the same manner by the same library in both parts of the project (the model building one and the model using one) for it to give correct results. But rather than defining the function everywhere, I thought of building a package that would contain this and other code that should be repetitively used. So I started reading about how to build package. Another thought that occurred to me was that I should generate documentation of the whole code, which can be later submitted as the project report. And the documentation should be automatically generated, with the help of comments in the code. For this purpose, I decided to use [Doxygen](http://doxygen.nl/). Doxygen was originally made for other languages and has recently added python in its set of languages, but guess who has been speaking python since long ago:

![](https://cdn.iwastesomuchtime.com/2182016205396.jpg)

20 points to Gryffindor!!! 
