# Day 14 of Summer Internship

*Date: June 20th, 2019*

After completing our mail server setup yesterday, I eagerly returned to our python code and started building the actual working model. At this point, we have completed 2 stages of the project:

1. Built and successfully tested the ML model
2. Successfully installed and configured the mail server

Now as per our modified plan (which I modified without informing you guys), we have the following parts left:

1. Create the code that would implement the ML model
2. Extensively test the code with real mails
3. Deploy the whole project

So I started building the code which would implement the ML model. Unlike the code that build the model, this one would read one mail at a time. As decided on the [9th day](https://hacksd.wordpress.com/2019/06/14/day-9-of-summer-internship/), it would have the following works to do:

1. Read the incoming mail.
1. Preprocess it.
1. Load the dictionary.
1. Extract features from that mail.
1. Load the ML model.
1. Classify the mail and put it in the respective folder (inbox or spam).

So starting with the preprocessor, I first decided that it would do the following to filter out the mail:

1. Check that all words contain only alphabets.
1. Check that length of word is greater than 2.
1. Remove [stop words](https://en.wikipedia.org/wiki/Stop_words), i.e the words that occur too frequently and don't lend much to the meaning to the sentence.
1. Perform [lemmatization](https://en.wikipedia.org/wiki/Lemmatisation), i.e change the word to its base form.

Now the first two parts were already happening in the model building code, so I copied those parts in our preprocessor. Now waht was left was removing stop words and lemmatization. For this, I searched the net and got [NLTK](https://www.nltk.org/)(Natural Language ToolKit) of python. It had its own collection of stop words and it even performs lemmatization. So initially I utilised it for the stop words purpose but then when I researched more, I found that its collection of English stop words has only 179 words. The length wasn't much of a problem but I decided to search for a better and more efficient list of stop words and possibly a better library. And something better than a [nlp](https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e) library is a nlp joke:

![](https://i.redd.it/ipy3vvkma2501.jpg)
